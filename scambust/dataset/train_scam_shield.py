import pandas as pd
import numpy as np
import io
import os
import pickle
from textblob import TextBlob
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# ---------------------------------------------------------
<<<<<<< HEAD
# INSTRUCTIONS:
# 1. Run: pip install pandas scikit-learn textblob scipy
# 2. Run: python advanced_scam_training.py
# ---------------------------------------------------------

# ---------------------------------------------------------
# 1. DATA LOAD & PREPROCESSING
=======
# INSTRUCTIONS
# 1.  pip install pandas scikit-learn
# 2.  python train_scam_shield.py
# 
# STEP 1: DATA LOADING
>>>>>>> 7b9e32aa5279a6cccd037b7652c38aa6a8b3a2c4
# ---------------------------------------------------------

# Priority: Unified > SMS
unified_file = "public_unified_multimodal.csv"
sms_file = "public_sms.csv"

<<<<<<< HEAD
df = None
text_column = ""
target_column = "is_scam"

if os.path.exists(unified_file):
    print(f"âœ… Found Unified Dataset: '{unified_file}'")
    df = pd.read_csv(unified_file)
    text_column = "text"
elif os.path.exists(sms_file):
    print(f"âš ï¸ Unified file not found. Using SMS Dataset: '{sms_file}'")
    df = pd.read_csv(sms_file)
    text_column = "message_text"
else:
    print("âŒ Error: No dataset found. Please upload 'public_unified_multimodal.csv'.")
    # Minimal mock data to prevent crash if file missing
    data = {'text': ["Free money click link", "Hi mom I am home"], 'is_scam': [1, 0]}
    df = pd.DataFrame(data)
    text_column = 'text'

# Drop rows with missing text or labels
df = df.dropna(subset=[text_column, target_column])

# --- DYNAMIC BALANCING & ADVERSARIAL TRAINING ---
scam_count = df[df[target_column] == 1].shape[0]
safe_count = df[df[target_column] == 0].shape[0]

print(f"Original Counts -> Scam: {scam_count}, Safe: {safe_count}")

# 1. Add Synthetic Safe Examples (Standard Balancing)
if safe_count < scam_count:
    print(f"âš–ï¸ Balancing Data: Adding synthetic safe examples...")
    safe_templates = [
        "Hi, I reached home safely.", "Call me when you are free.", "The meeting is at 4 PM.",
        "Ok, I will bring the groceries.", "Love you mom, see you soon.", "Good morning!",
        "Your order #12345 has been delivered.", "Your OTP is 1234 (Generated by User).", 
        "Transaction successful: Rs 200 paid to Zomato.", "Your cab is arriving in 5 mins."
    ]
    needed = scam_count - safe_count
    multiplier = (needed // len(safe_templates)) + 1
    synthetic_safe_texts = (safe_templates * multiplier)[:needed]
    extra_safe_df = pd.DataFrame({text_column: synthetic_safe_texts, target_column: 0})
    df = pd.concat([df, extra_safe_df], ignore_index=True)

# 2. Add Adversarial Examples (Tricky Cases to fix logic)
# These teach the model that short commands or financial keywords are scams
adversarial_data = {
    text_column: [
        "click on the link", "click this link", "paye reward", "claim reward", 
        "pay 500", "send money", "verify kyc", "account blocked", 
        "lottery winner", "urgent pay", "scam alert", "this is a scam"
    ],
    target_column: [
        1, 1, 1, 1, 
        1, 1, 1, 1, 
        1, 1, 0, 0  # "scam alert" or discussing scams is usually safe/info
    ]
}
# Add these multiple times to give them weight
adv_df = pd.DataFrame(adversarial_data)
adv_df_weighted = pd.concat([adv_df] * 5, ignore_index=True) 
df = pd.concat([df, adv_df_weighted], ignore_index=True)

print(f"Enhanced Dataset Size: {len(df)}")

# ---------------------------------------------------------
# 2. FEATURE ENGINEERING (NLP Metrics)
# ---------------------------------------------------------
print("ðŸ“Š Extracting NLP Features (Sentiment, Complexity)...")

def get_text_metrics(text):
    blob = TextBlob(str(text))
    words = str(text).split()
    return pd.Series({
        'polarity': blob.sentiment.polarity,       # Sentiment: -1 (Negative) to 1 (Positive)
        'subjectivity': blob.sentiment.subjectivity, # 0 (Objective) to 1 (Subjective)
        'word_count': len(words),
        'avg_word_len': np.mean([len(w) for w in words]) if len(words) > 0 else 0
    })

# Apply extraction
metrics_df = df[text_column].apply(get_text_metrics)

# ---------------------------------------------------------
# 3. VECTORIZATION & FEATURE COMBINATION
# ---------------------------------------------------------
print("ðŸ”¤ Vectorizing Text (TF-IDF)...")

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(
    max_features=5000, 
    ngram_range=(1, 3), # Increased n-gram range to capture "click on the link"
    stop_words='english',
    analyzer='word' 
)
X_text = tfidf.fit_transform(df[text_column].astype(str))

# Combine Text Vectors with Numerical NLP Features
X_final = hstack([X_text, metrics_df.values]) 
y = df[target_column]

# ---------------------------------------------------------
# 4. MODEL TRAINING (Ensemble: LR + RF)
# ---------------------------------------------------------
print("ðŸ¤– Training Ensemble Models...")

X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Model 1: Logistic Regression (Good for sparse text data)
lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print(f"âœ… Logistic Regression Accuracy: {accuracy_score(y_test, lr_pred):.4f}")

# Model 2: Random Forest (Good for numerical/non-linear patterns)
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
print(f"âœ… Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}")

# ---------------------------------------------------------
# 5. SAVE ARTIFACTS
# ---------------------------------------------------------
print("ðŸ’¾ Saving Models...")

with open('scam_detection_lr.pkl', 'wb') as f:
    pickle.dump(lr_model, f)

with open('scam_detection_rf.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

print("ðŸŽ‰ Training Complete! Models saved.")

# ---------------------------------------------------------
# 6. HYBRID PREDICTION FUNCTION
# ---------------------------------------------------------
def predict_scam_hybrid(message):
    # 1. Extract Features
    metrics = get_text_metrics(message)
    # 2. Vectorize Text
    text_vector = tfidf.transform([message])
    # 3. Combine
    features = hstack([text_vector, [metrics.values]])
    
    # 4. Predict Probabilities
    lr_prob = lr_model.predict_proba(features)[0][1] # Probability of being scam (class 1)
    rf_prob = rf_model.predict_proba(features)[0][1]
    
    # 5. Ensemble (Average Probability)
    final_prob = (lr_prob + rf_prob) / 2
    
    # Refined Threshold Logic
    if final_prob > 0.6:
        label = "ðŸš¨ SCAM"
    elif final_prob > 0.3:
        label = "âš ï¸ SUSPICIOUS"
    else:
        label = "âœ… SAFE"
        
    return f"{label} (Risk: {final_prob*100:.1f}%)"
=======
if os.path.exists(file_name):
    print(f"'{file_name}' Found.")
    df = pd.read_csv(file_name)
    
    # NOTE: Agar aapki csv file mein columns ke naam alag hain, toh rename karein.
    # Niche wali line check kar lena dataset ke hisaab se:
    # df = df.rename(columns={'v2': 'message_text', 'v1': 'is_scam'})

else:
    print("file not found !")
    csv_data = """sms_id,message_text,is_scam
SMS001,TRAI verification pending. Confirm details by calling +91-98728-31517.,1
SMS002,Address incomplete. Reschedule delivery: https://rebrand.ly/GdfOiZ6 Jaldi karo.,1
SMS003,PhonePe Security: Malware detected. Install Remote Assist from https://cybercrime-case.in,1
SMS004,Hi, Iâ€™ll reach by 7:00 PM. See you soon.,0
SMS005,Device warranty expiring today. Verify at Jaldi karo.,1
SMS006,URGENT: Jio password change hua. Agar aapne nahi kiya to reset: https://amazon.in,0
SMS007,Apple Security: Malware detected. Install QuickSupport from https://verify-now.co,1
SMS008,BESCOM: Bill overdue â‚¹7500. Power cut in 2 hours. Pay at https://bit.ly/effIp1B,1
SMS009,OLX: Buyer sent payment. To receive â‚¹50000, share OTP.,1
SMS010,Delhivery: Delivery attempt failed. Reschedule at https://t.co/8Kjuhfj,1
SMS011,KYC update required. Share PAN/Aadhaar photo on WhatsApp.,1
SMS012,Prize release requires processing fee â‚¹99. Pay via UPI.,1
SMS013,Dear customer, your card will be blocked today. Verify details at https://rebrand.ly,1
SMS014,Reminder: Appointment with Dr. Mehta on 03-Nov at 7:00 PM.,0
SMS015,Hi Mom, new number. Emergency. Send â‚¹1500 to UPI.,1
SMS016,Blue Dart: Delivery attempt failed. Reschedule at https://rebrand.ly,1
SMS017,Hi, Iâ€™ll reach by 10:00 AM. See you soon.,0
SMS018,HDFC: Rs.50000 debited from A/C XX8082. Avl bal: Rs.45000.,0
SMS019,DTDC: Delivery attempt fail. Reschedule: https://tinyurl.com,1
SMS020,Earn â‚¹100000/day. Join VIP group using https://bit.ly,1
"""
    df = pd.read_csv(io.StringIO(csv_data))

print(df.head())
print(f"\nTotal messages loaded: {len(df)}")

# ---------------------------------------------------------
# STEP 2: MODEL TRAINING 
# ---------------------------------------------------------

if 'message_text' not in df.columns:
    print("\nError : 'message_text' column not found")
    # if col has some different name, trying  to find it
    text_cols = [col for col in df.columns if 'text' in col.lower() or 'msg' in col.lower() or 'message' in col.lower()]
    if text_cols:
        print(f"your column can be  '{text_cols[0]}'")
        X = df[text_cols[0]]
    else:
        print("Column nahi mila. Code band ho raha hai.")
        exit()
else:
    X = df['message_text']

# Target column dhoondna (is_scam, label, etc)
if 'is_scam' in df.columns:
    y = df['is_scam']
elif 'label' in df.columns:
    y = df['label']
else:
    print("Target column not found.")
    exit()


# training the model
model = make_pipeline(CountVectorizer(), MultinomialNB())
model.fit(X, y)
print("âœ… Model training complete!")
>>>>>>> 7b9e32aa5279a6cccd037b7652c38aa6a8b3a2c4

# ---------------------------------------------------------
# 7. INTERACTIVE TESTING
# ---------------------------------------------------------
<<<<<<< HEAD
print("\n" + "="*50)
print("ðŸ” ADVANCED SCAM CHECKER")
print("Type a message to check. Type 'exit' to quit.")
print("="*50)

while True:
    try:
        user_input = input("\nðŸ“ Enter message: ")
        if user_input.lower() in ['exit', 'quit']:
            break
        if not user_input.strip():
            continue
            
        result = predict_scam_hybrid(user_input)
        print(f"Result: {result}")
        
    except KeyboardInterrupt:
        break
    except Exception as e:
        print(f"Error: {e}")
=======
model_filename = 'scam_model.pkl'
with open(model_filename, 'wb') as f:
    pickle.dump(model, f)
# model is saved as 'scam_model.pkl' , we wil use it in future for predictions

# ---------------------------------------------------------
# STEP 4: TESTING
# ---------------------------------------------------------

def check_scam(text_message):
    prediction = model.predict([text_message])[0]
    if prediction == 1:
        return "SCAM ALERT! (Savdhaan Rahein)"
    else:
        return " Safe Message"

print("\n--- Live Testing Results (Naye Messages) ---")
test_messages = [
    "Tera bhai ghar pahuch gaya hai.", 
    "Urgent! Your electricity will be cut. Pay 500rs at link http://fake.com",
    "Hello mom, send me money urgent UPI.",
    "Your appointment is confirmed for Monday." 
]

for msg in test_messages:
    result = check_scam(msg)
    print(f"Message: '{msg}'\nResult: {result}\n")

print("--- Apna khud ka message check karein ---")
try:
    user_msg = input("Koi message type karein aur Enter dabayein: ")
    if user_msg:
        print(f"Result: {check_scam(user_msg)}")
except:
    pass
>>>>>>> 7b9e32aa5279a6cccd037b7652c38aa6a8b3a2c4
