import pandas as pd
import numpy as np
import io
import os
import pickle
from textblob import TextBlob
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# ---------------------------------------------------------
# INSTRUCTIONS:
# 1. Run: pip install pandas scikit-learn textblob scipy
# 2. Run: python advanced_scam_training.py
# ---------------------------------------------------------

# ---------------------------------------------------------
# 1. DATA LOAD & PREPROCESSING
# ---------------------------------------------------------

# Priority: Unified > SMS
unified_file = "public_unified_multimodal.csv"
sms_file = "public_sms.csv"

df = None
text_column = ""
target_column = "is_scam"

if os.path.exists(unified_file):
    print(f"‚úÖ Found Unified Dataset: '{unified_file}'")
    df = pd.read_csv(unified_file)
    text_column = "text"
elif os.path.exists(sms_file):
    print(f"‚ö†Ô∏è Unified file not found. Using SMS Dataset: '{sms_file}'")
    df = pd.read_csv(sms_file)
    text_column = "message_text"
else:
    print("‚ùå Error: No dataset found. Please upload 'public_unified_multimodal.csv'.")
    # Minimal mock data to prevent crash if file missing
    data = {'text': ["Free money click link", "Hi mom I am home"], 'is_scam': [1, 0]}
    df = pd.DataFrame(data)
    text_column = 'text'

# Drop rows with missing text or labels
df = df.dropna(subset=[text_column, target_column])

# --- DYNAMIC BALANCING & ADVERSARIAL TRAINING ---
scam_count = df[df[target_column] == 1].shape[0]
safe_count = df[df[target_column] == 0].shape[0]

print(f"Original Counts -> Scam: {scam_count}, Safe: {safe_count}")

# 1. Add Synthetic Safe Examples (Standard Balancing)
if safe_count < scam_count:
    print(f"‚öñÔ∏è Balancing Data: Adding synthetic safe examples...")
    safe_templates = [
        "Hi, I reached home safely.", "Call me when you are free.", "The meeting is at 4 PM.",
        "Ok, I will bring the groceries.", "Love you mom, see you soon.", "Good morning!",
        "Your order #12345 has been delivered.", "Your OTP is 1234 (Generated by User).", 
        "Transaction successful: Rs 200 paid to Zomato.", "Your cab is arriving in 5 mins."
    ]
    needed = scam_count - safe_count
    multiplier = (needed // len(safe_templates)) + 1
    synthetic_safe_texts = (safe_templates * multiplier)[:needed]
    extra_safe_df = pd.DataFrame({text_column: synthetic_safe_texts, target_column: 0})
    df = pd.concat([df, extra_safe_df], ignore_index=True)

# 2. Add Adversarial Examples (Tricky Cases to fix logic)
# These teach the model that short commands or financial keywords are scams
adversarial_data = {
    text_column: [
        "click on the link", "click this link", "paye reward", "claim reward", 
        "pay 500", "send money", "verify kyc", "account blocked", 
        "lottery winner", "urgent pay", "scam alert", "this is a scam"
    ],
    target_column: [
        1, 1, 1, 1, 
        1, 1, 1, 1, 
        1, 1, 0, 0  # "scam alert" or discussing scams is usually safe/info
    ]
}
# Add these multiple times to give them weight
adv_df = pd.DataFrame(adversarial_data)
adv_df_weighted = pd.concat([adv_df] * 5, ignore_index=True) 
df = pd.concat([df, adv_df_weighted], ignore_index=True)

print(f"Enhanced Dataset Size: {len(df)}")

# ---------------------------------------------------------
# 2. FEATURE ENGINEERING (NLP Metrics)
# ---------------------------------------------------------
print("üìä Extracting NLP Features (Sentiment, Complexity)...")

def get_text_metrics(text):
    blob = TextBlob(str(text))
    words = str(text).split()
    return pd.Series({
        'polarity': blob.sentiment.polarity,       # Sentiment: -1 (Negative) to 1 (Positive)
        'subjectivity': blob.sentiment.subjectivity, # 0 (Objective) to 1 (Subjective)
        'word_count': len(words),
        'avg_word_len': np.mean([len(w) for w in words]) if len(words) > 0 else 0
    })

# Apply extraction
metrics_df = df[text_column].apply(get_text_metrics)

# ---------------------------------------------------------
# 3. VECTORIZATION & FEATURE COMBINATION
# ---------------------------------------------------------
print("üî§ Vectorizing Text (TF-IDF)...")

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(
    max_features=5000, 
    ngram_range=(1, 3), # Increased n-gram range to capture "click on the link"
    stop_words='english',
    analyzer='word' 
)
X_text = tfidf.fit_transform(df[text_column].astype(str))

# Combine Text Vectors with Numerical NLP Features
X_final = hstack([X_text, metrics_df.values]) 
y = df[target_column]

# ---------------------------------------------------------
# 4. MODEL TRAINING (Ensemble: LR + RF)
# ---------------------------------------------------------
print("ü§ñ Training Ensemble Models...")

X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Model 1: Logistic Regression (Good for sparse text data)
lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print(f"‚úÖ Logistic Regression Accuracy: {accuracy_score(y_test, lr_pred):.4f}")

# Model 2: Random Forest (Good for numerical/non-linear patterns)
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
print(f"‚úÖ Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}")

# ---------------------------------------------------------
# 5. SAVE ARTIFACTS
# ---------------------------------------------------------
print("üíæ Saving Models...")

with open('scam_detection_lr.pkl', 'wb') as f:
    pickle.dump(lr_model, f)

with open('scam_detection_rf.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

print("üéâ Training Complete! Models saved.")

# ---------------------------------------------------------
# 6. HYBRID PREDICTION FUNCTION
# ---------------------------------------------------------
def predict_scam_hybrid(message):
    # 1. Extract Features
    metrics = get_text_metrics(message)
    # 2. Vectorize Text
    text_vector = tfidf.transform([message])
    # 3. Combine
    features = hstack([text_vector, [metrics.values]])
    
    # 4. Predict Probabilities
    lr_prob = lr_model.predict_proba(features)[0][1] # Probability of being scam (class 1)
    rf_prob = rf_model.predict_proba(features)[0][1]
    
    # 5. Ensemble (Average Probability)
    final_prob = (lr_prob + rf_prob) / 2
    
    # Refined Threshold Logic
    if final_prob > 0.6:
        label = "üö® SCAM"
    elif final_prob > 0.3:
        label = "‚ö†Ô∏è SUSPICIOUS"
    else:
        label = "‚úÖ SAFE"
        
    return f"{label} (Risk: {final_prob*100:.1f}%)"

# ---------------------------------------------------------
# 7. INTERACTIVE TESTING
# ---------------------------------------------------------
print("\n" + "="*50)
print("üîç ADVANCED SCAM CHECKER")
print("Type a message to check. Type 'exit' to quit.")
print("="*50)

while True:
    try:
        user_input = input("\nüìù Enter message: ")
        if user_input.lower() in ['exit', 'quit']:
            break
        if not user_input.strip():
            continue
            
        result = predict_scam_hybrid(user_input)
        print(f"Result: {result}")
        
    except KeyboardInterrupt:
        break
    except Exception as e:
        print(f"Error: {e}")